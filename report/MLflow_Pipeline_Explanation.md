# MLflow Experiments & Data Pipeline Explanation

## Project: Sentiment Analysis with Databricks & MLflow

---

## ğŸ—ï¸ The Data Pipeline (Bronze â†’ Silver â†’ Gold)

This project uses **"Medallion Architecture"** - a standard data engineering pattern for data lakes.

### 1. **Bronze Layer** (Raw Data)
- **Input**: TSV files uploaded to `/Volumes/workspace/sentiment_analysis/raw`
- **Process**: `bronze_data_ingestion.ipynb` reads TSV â†’ adds metadata
- **Output**: Parquet files in `/Volumes/workspace/sentiment_analysis/bronze`
- **Purpose**: "Just dump everything as-is" - preserve raw data
- **MLflow Experiment**: `bronze_data_ingestion` + `bronze_validation`

**What gets logged to MLflow:**
- Row count from source
- Column count
- Distinct reviews & products
- Date ranges
- Null counts per column
- Duplicate detection
- Schema artifacts

### 2. **Silver Layer** (Cleaned Data)
- **Input**: Bronze data
- **Process**: `silver_data_ingestion.ipynb` performs:
  - Remove nulls in required fields
  - Fix data types (star_rating â†’ float, dates â†’ timestamp)
  - Remove duplicates by review_id
  - Filter invalid ratings (keep only 1-5 stars)
- **Output**: Clean data in `/Volumes/workspace/sentiment_analysis/silver`
- **Purpose**: "Clean up the mess" - ensure data quality
- **MLflow Experiment**: `silver_data_ingestion`

**What gets logged to MLflow:**
- Bronze row count
- Silver row count
- Retention ratio (% of data kept after cleaning)
- Invalid rows dropped
- Process timestamp

### 3. **Gold Layer** (ML-Ready Data)
- **Input**: Silver data
- **Process**: `gold_data_ingestion_sa.ipynb` performs:
  - Text cleaning (lowercase, remove HTML, regex cleaning)
  - Create sentiment labels:
    - 1-2 stars â†’ "negative"
    - 3 stars â†’ "neutral"
    - 4-5 stars â†’ "positive"
  - Feature engineering (word count, character count, etc.)
  - Select ML-relevant columns
- **Output**: ML-ready data in `/Volumes/workspace/sentiment_analysis/gold`
- **Purpose**: "Make it perfect for machine learning"
- **MLflow Experiment**: `gold_data_ingestion`

**What gets logged to MLflow:**
- Total rows & columns
- Sentiment label distribution (negative/neutral/positive ratios)
- Average review length
- Missing data ratios
- Completeness score (data quality metric)
- Class balance metrics

### 4. **Model Training** (Uses Gold Data)
- **Input**: Gold data (clean text + sentiment labels)
- **Process**: `sentiment_analysis.ipynb` or `model_train.py`:
  1. Load Gold data
  2. Vectorize text using TF-IDF (50,000 features, unigrams + bigrams)
  3. Split: 80% train, 20% test (stratified)
  4. Train 12 model variants
  5. Evaluate each model
  6. Log everything to MLflow
- **Output**: 12 trained models with metrics
- **MLflow Experiment**: `sentiment_analysis`

**What gets logged to MLflow (per model):**
- Model name
- Hyperparameters (C, alpha, max_iter, etc.)
- F1 Score (macro-averaged) - **PRIMARY METRIC**
- Precision (macro-averaged)
- Recall (macro-averaged)
- Trained model artifacts (saved models)

---

## ğŸ¤– The 12 Models - Detailed Breakdown

**Source**: Lines 80-100 in `src/model_train.py`

### **Group 1: Logistic Regression (3 variants)**

Logistic Regression with One-vs-Rest classification, testing different regularization strengths:

| Model Name | Algorithm | C Value | Description |
|------------|-----------|---------|-------------|
| `LR_C1` | LogisticRegression | C=1 | Lower regularization (more penalty) |
| `LR_C5` | LogisticRegression | C=5 | Medium regularization |
| `LR_C10` | LogisticRegression | C=10 | Higher regularization (less penalty) |

**Common settings:**
- `max_iter=5000`
- `solver="liblinear"`
- `class_weight="balanced"` (handles imbalanced classes)

**Purpose**: Test how much regularization helps with high-dimensional text data.

---

### **Group 2: Linear SVM (4 variants)**

Linear Support Vector Machine with One-vs-Rest, testing a wider range of C values:

| Model Name | Algorithm | C Value | Description |
|------------|-----------|---------|-------------|
| `SVM_OVR_C01` | LinearSVC | C=0.1 | Very strong regularization |
| `SVM_OVR_C1` | LinearSVC | C=1.0 | Medium regularization |
| `SVM_OVR_C10` | LinearSVC | C=10.0 | Weak regularization |
| `SVM_OVR_C50` | LinearSVC | C=50.0 | Very weak regularization |

**Common settings:**
- `max_iter=5000`
- Wrapped in `OneVsRestClassifier` for multi-class

**Purpose**: SVMs often work well for text classification. Testing different regularization to find optimal balance.

---

### **Group 3: Multinomial Naive Bayes (4 variants)**

Naive Bayes with different smoothing parameters:

| Model Name | Algorithm | Alpha Value | Description |
|------------|-----------|-------------|-------------|
| `NB_alpha02` | MultinomialNB | Î±=0.2 | Minimal smoothing |
| `NB_alpha05` | MultinomialNB | Î±=0.5 | Less smoothing |
| `NB_alpha1` | MultinomialNB | Î±=1.0 | Standard Laplace smoothing |
| `NB_alpha2` | MultinomialNB | Î±=2.0 | More smoothing |

**Purpose**: Naive Bayes is fast and works well with text. Alpha controls smoothing to handle words not seen in training.

---

## ğŸ”„ Complete Data Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     DATA PIPELINE FLOW                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“ Raw TSV Files
   (Your uploaded data: ~15M product reviews)
        â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  bronze_data_ingestion.ipynb        â”‚
   â”‚  - Read TSV with schema             â”‚
   â”‚  - Add metadata (timestamp, source) â”‚
   â”‚  - Log: rows, columns, schema       â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
ğŸ“¦ BRONZE Layer
   /Volumes/workspace/sentiment_analysis/bronze
   - Raw data preserved
   - ~15M rows (or sampled to 40% for free tier)
   - All original columns
        â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  silver_data_ingestion.ipynb        â”‚
   â”‚  - Remove nulls                     â”‚
   â”‚  - Fix data types                   â”‚
   â”‚  - Remove duplicates                â”‚
   â”‚  - Filter invalid ratings           â”‚
   â”‚  - Log: retention ratio             â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
ğŸ§¹ SILVER Layer
   /Volumes/workspace/sentiment_analysis/silver
   - Cleaned data
   - ~10-12M rows (depending on data quality)
   - Standardized column names
        â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  gold_data_ingestion_sa.ipynb       â”‚
   â”‚  - Clean text (lowercase, HTML)     â”‚
   â”‚  - Map stars â†’ sentiment labels     â”‚
   â”‚  - Feature engineering              â”‚
   â”‚  - Log: class distribution          â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
âœ¨ GOLD Layer
   /Volumes/workspace/sentiment_analysis/gold
   - ML-ready features
   - Columns: review_id, product_id, clean_text, sentiment_label
   - Labels: negative (1-2â˜…), neutral (3â˜…), positive (4-5â˜…)
        â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  sentiment_analysis.ipynb           â”‚
   â”‚  OR model_train.py                  â”‚
   â”‚  - Load gold data                   â”‚
   â”‚  - TF-IDF vectorization             â”‚
   â”‚  - Train 12 models                  â”‚
   â”‚  - Evaluate & log metrics           â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
ğŸ¤– 12 TRAINED MODELS
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ 3 Logistic Regression variants  â”‚
   â”‚  - LR_C1, LR_C5, LR_C10        â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ 4 Linear SVM variants           â”‚
   â”‚  - SVM_OVR_C01, SVM_OVR_C1,    â”‚
   â”‚    SVM_OVR_C10, SVM_OVR_C50    â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ 4 Naive Bayes variants          â”‚
   â”‚  - NB_alpha02, NB_alpha05,     â”‚
   â”‚    NB_alpha1, NB_alpha2        â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
ğŸ“Š MLflow Tracking
   - All 12 runs logged
   - Metrics: F1, Precision, Recall
   - Artifacts: Trained models saved
   - Best model identified
```

---

## ğŸ¯ Why 12 Models? (Hyperparameter Search)

This is called **hyperparameter tuning** or **model selection**. The goal is to:

1. **Test different algorithms**
   - Logistic Regression (linear, probabilistic)
   - SVM (maximum margin classifier)
   - Naive Bayes (probabilistic, assumes feature independence)

2. **Test different hyperparameters**
   - **C** (for LR & SVM): Controls regularization strength
     - Lower C = More regularization = Simpler model
     - Higher C = Less regularization = More complex model
   - **alpha** (for Naive Bayes): Controls smoothing
     - Lower alpha = Less smoothing = Trust training data more
     - Higher alpha = More smoothing = More conservative

3. **Find the best combination**
   - Compare all 12 based on F1 score (macro-averaged)
   - F1 balances precision and recall
   - Macro-average treats all classes equally (good for imbalanced data)

**This is proper MLOps!** Instead of guessing, you systematically test and track everything.

---

## ğŸ“Š Your MLflow Experiments

When you look at **Machine Learning â†’ Experiments** in Databricks, you see:

### Experiment: `bronze_data_ingestion`
- **Purpose**: Track raw data ingestion
- **Runs**: 1 per execution
- **Metrics**: rows_read, columns

### Experiment: `bronze_validation`
- **Purpose**: Track data quality checks
- **Runs**: 1 per execution
- **Metrics**: row_count, distinct_reviews, distinct_products, duplicate_reviews

### Experiment: `silver_data_ingestion`
- **Purpose**: Track data cleaning
- **Runs**: 1 per execution
- **Metrics**: retention_ratio, invalid_rows, bronze_rows, silver_rows

### Experiment: `gold_data_ingestion`
- **Purpose**: Track feature engineering
- **Runs**: 1 per execution
- **Metrics**: label_ratio_negative, label_ratio_neutral, label_ratio_positive, completeness_score, avg_review_length

### Experiment: `sentiment_analysis` â­ (MAIN EXPERIMENT)
- **Purpose**: Track model training & comparison
- **Runs**: 12 (one per model variant)
- **Metrics per run**:
  - `f1_score` (PRIMARY - used for model selection)
  - `precision`
  - `recall`
- **Parameters per run**:
  - `model_name` (e.g., "LR_C5", "SVM_OVR_C10")
  - Hyperparameters (C, alpha)
- **Artifacts**: Trained model files

---

## ğŸ“ Key Concepts Explained

### What is MLflow?
- **Experiment tracking system** built into Databricks
- Logs parameters, metrics, and model artifacts automatically
- Allows comparison of multiple runs
- Essential for reproducible ML

### What is Unity Catalog?
- **Enterprise data catalog** for Databricks
- Organizes data in: Catalog â†’ Schema â†’ Tables/Volumes
- Your structure: `workspace.sentiment_analysis.{raw,bronze,silver,gold}`
- Provides governance and access control

### What is the Medallion Architecture?
- **Best practice** for organizing data lakes
- **Bronze** = Raw, immutable data
- **Silver** = Cleaned, deduplicated data
- **Gold** = Business-level, aggregated data (ML-ready)
- Benefits: Data quality, traceability, reprocessing

### What is Hyperparameter Tuning?
- **Systematic search** for best model settings
- Instead of guessing, test multiple configurations
- Track everything with MLflow
- Select best based on objective metric (F1 score)

---

## ğŸ’¡ How to Use This in Your TP Report

### 1. Architecture Diagram
Use the flow diagram above to explain your data pipeline.

### 2. Experiment Results Table
Export from MLflow:

| Model | F1 Score | Precision | Recall | Best? |
|-------|----------|-----------|--------|-------|
| LR_C1 | 0.XXXX | 0.XXXX | 0.XXXX | |
| LR_C5 | 0.XXXX | 0.XXXX | 0.XXXX | âœ“ |
| ... | ... | ... | ... | |

### 3. Key Metrics to Report
- **Data volume**: Rows at each stage (Bronze â†’ Silver â†’ Gold)
- **Data quality**: Retention ratio, duplicate percentage
- **Model performance**: Best F1 score, comparison across algorithms
- **Reproducibility**: All experiments tracked in MLflow

### 4. MLOps Concepts Demonstrated
- âœ… Experiment tracking (MLflow)
- âœ… Data versioning (Medallion architecture)
- âœ… Model registry (Unity Catalog)
- âœ… Hyperparameter tuning (12 variants)
- âœ… Automated pipelines (notebooks)
- âœ… Reproducibility (all parameters logged)

---

## ğŸš€ Summary

**You have successfully implemented a complete MLOps pipeline!**

1. âœ… **Data Pipeline**: Bronze â†’ Silver â†’ Gold (Medallion Architecture)
2. âœ… **Model Training**: 12 variants with systematic hyperparameter search
3. âœ… **Experiment Tracking**: All metrics logged to MLflow
4. âœ… **Model Registry**: Best model saved to Unity Catalog
5. âœ… **Reproducibility**: Every step tracked and documented

**This is enterprise-grade MLOps**, suitable for production environments!

---

*Generated for: TP - Herramientas para Grandes VolÃºmenes de Datos*
*Student: nmoccagatta@itba.edu.ar*
*Date: 2025-12-11*
