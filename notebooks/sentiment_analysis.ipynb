{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92877c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leandrohermann/Library/CloudStorage/OneDrive-Personal/ITBA/Big Data/tp/itba-bigdata/.venv/lib/python3.13/site-packages/mlflow/tracking/_tracking_service/utils.py:140: FutureWarning: Filesystem tracking backend (e.g., './mlruns') is deprecated. Please switch to a database backend (e.g., 'sqlite:///mlflow.db'). For feedback, see: https://github.com/mlflow/mlflow/issues/18534\n",
      "  return FileStore(store_uri, store_uri)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location=('file:///Users/leandrohermann/Library/CloudStorage/OneDrive-Personal/ITBA/Big '\n",
       " 'Data/tp/itba-bigdata/notebooks/../mlruns/206939664416179806'), creation_time=1763089599312, experiment_id='206939664416179806', last_update_time=1763089599312, lifecycle_stage='active', name='sentiment_baselines', tags={'mlflow.experimentKind': 'custom_model_development', 'step': 'model_training'}>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "import mlflow\n",
    "\n",
    "# Paths and constants\n",
    "DATA_PATH = Path(\n",
    "    \"../data/gold/sentiment/amazon_reviews_furniture/sentiment.parquet/\")\n",
    "EXPERIMENT_NAME = \"sentiment_baselines\"\n",
    "MLFLOW_URI = \"file:../mlruns\"\n",
    "RANDOM_SEED = 42\n",
    "TARGET_COL = \"sentiment_label\"\n",
    "TEXT_COL = \"clean_text\"\n",
    "\n",
    "# Model save locations\n",
    "OUTPUT_DIR = Path(\"../models\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "mlflow.set_tracking_uri(MLFLOW_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e68d2478",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/07 21:14:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "import logging\n",
    "logging.basicConfig(level=logging.WARN)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SentimentAnalysisPrototype\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "logger.info(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "031768ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- clean_text: string (nullable = true)\n",
      " |-- sentiment_label: string (nullable = true)\n",
      " |-- star_rating: integer (nullable = true)\n",
      " |-- review_date: date (nullable = true)\n",
      " |-- helpful_votes: integer (nullable = true)\n",
      " |-- total_votes: integer (nullable = true)\n",
      " |-- verified_purchase: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "logger.info(f\"Loading parquet from {DATA_PATH}\")\n",
    "df = spark.read.parquet(str(DATA_PATH))\n",
    "logger.info(f\"Rows: {df.count()}, Columns: {len(df.columns)}\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "287c8fb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R1000ZEHB3O259</td>\n",
       "      <td>B0044WWL34</td>\n",
       "      <td>using on a front hall throw rug and the grippi...</td>\n",
       "      <td>positive</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-03-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R1002K4NKXO8VI</td>\n",
       "      <td>B00166GCJ0</td>\n",
       "      <td>it's a cheap chair. it doesn't cost much and i...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3</td>\n",
       "      <td>2009-08-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R1002MW05H9LI</td>\n",
       "      <td>B00EQG7KEI</td>\n",
       "      <td>wonderful , sturdy , easy to set up. no box sp...</td>\n",
       "      <td>positive</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R10034VWRCN7PM</td>\n",
       "      <td>B006JG0HMK</td>\n",
       "      <td>i had 3 issues with the product upon receipt. ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-06-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R1003EZF56E6JB</td>\n",
       "      <td>B0042D8VRU</td>\n",
       "      <td>the ups guy said this was the heaviest item he...</td>\n",
       "      <td>positive</td>\n",
       "      <td>5</td>\n",
       "      <td>2012-12-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>R1003L1ICGH3Y8</td>\n",
       "      <td>B00E44V6II</td>\n",
       "      <td>nice. i have not had a wake up call from someo...</td>\n",
       "      <td>positive</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-01-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>R10046A1NGXKTX</td>\n",
       "      <td>B00JJZ18PS</td>\n",
       "      <td>been 1 week and my sleep has been better. the ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>4</td>\n",
       "      <td>2014-08-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>R1004LAAZLOLYN</td>\n",
       "      <td>B00WEYR8KU</td>\n",
       "      <td>this is just ok for the price. you can take th...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3</td>\n",
       "      <td>2015-07-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>R1004ZAILG13HW</td>\n",
       "      <td>B00E8ONKB0</td>\n",
       "      <td>making a fleece tie blanket with the material....</td>\n",
       "      <td>positive</td>\n",
       "      <td>5</td>\n",
       "      <td>2014-10-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>R1005692J177UC</td>\n",
       "      <td>B008KP6D2G</td>\n",
       "      <td>great looking rug, a little thin but overall r...</td>\n",
       "      <td>positive</td>\n",
       "      <td>4</td>\n",
       "      <td>2015-05-15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        review_id  product_id  \\\n",
       "0  R1000ZEHB3O259  B0044WWL34   \n",
       "1  R1002K4NKXO8VI  B00166GCJ0   \n",
       "2   R1002MW05H9LI  B00EQG7KEI   \n",
       "3  R10034VWRCN7PM  B006JG0HMK   \n",
       "4  R1003EZF56E6JB  B0042D8VRU   \n",
       "5  R1003L1ICGH3Y8  B00E44V6II   \n",
       "6  R10046A1NGXKTX  B00JJZ18PS   \n",
       "7  R1004LAAZLOLYN  B00WEYR8KU   \n",
       "8  R1004ZAILG13HW  B00E8ONKB0   \n",
       "9  R1005692J177UC  B008KP6D2G   \n",
       "\n",
       "                                          clean_text sentiment_label  \\\n",
       "0  using on a front hall throw rug and the grippi...        positive   \n",
       "1  it's a cheap chair. it doesn't cost much and i...         neutral   \n",
       "2  wonderful , sturdy , easy to set up. no box sp...        positive   \n",
       "3  i had 3 issues with the product upon receipt. ...        negative   \n",
       "4  the ups guy said this was the heaviest item he...        positive   \n",
       "5  nice. i have not had a wake up call from someo...        positive   \n",
       "6  been 1 week and my sleep has been better. the ...        positive   \n",
       "7  this is just ok for the price. you can take th...         neutral   \n",
       "8  making a fleece tie blanket with the material....        positive   \n",
       "9  great looking rug, a little thin but overall r...        positive   \n",
       "\n",
       "   star_rating review_date  \n",
       "0            5  2015-03-20  \n",
       "1            3  2009-08-27  \n",
       "2            5  2015-07-23  \n",
       "3            1  2015-06-30  \n",
       "4            5  2012-12-23  \n",
       "5            5  2015-01-14  \n",
       "6            4  2014-08-13  \n",
       "7            3  2015-07-28  \n",
       "8            5  2014-10-19  \n",
       "9            4  2015-05-15  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick preview and basic quality checks\n",
    "display_cols = [\"review_id\", \"product_id\", \"clean_text\",\n",
    "                \"sentiment_label\", \"star_rating\", \"review_date\"]\n",
    "df.select(*display_cols).limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe0cee70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/07 21:14:59 WARN StopWordsRemover: Default locale set was [en_AR]; however, it was not found in available locales in JVM, falling back to en_US locale. Set param `locale` in order to respect another locale.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------\n",
    "# Baseline Experiments: LR vs SVM vs Naive Bayes\n",
    "# ----------------------------------------------------\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, LinearSVC, NaiveBayes, OneVsRest\n",
    "from pyspark.ml.feature import (\n",
    "    RegexTokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer\n",
    ")\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "\n",
    "# Common preprocessing stages\n",
    "regex_tokenizer = RegexTokenizer(\n",
    "    inputCol=\"clean_text\", outputCol=\"tokens\", pattern=\"\\\\W+\"\n",
    ")\n",
    "stop_remover = StopWordsRemover(\n",
    "    inputCol=\"tokens\", outputCol=\"tokens_nostop\"\n",
    ")\n",
    "hashing_tf = HashingTF(\n",
    "    inputCol=\"tokens_nostop\", outputCol=\"rawFeatures\", numFeatures=2**16\n",
    ")\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "label_indexer = StringIndexer(\n",
    "    inputCol=\"sentiment_label\", outputCol=\"label_idx\", handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "# Train/test split\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=RANDOM_SEED)\n",
    "\n",
    "# Evaluation function\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label_idx\", predictionCol=\"prediction\", metricName=\"f1\"\n",
    ")\n",
    "\n",
    "\n",
    "def run_experiment(model_name, classifier, params: dict):\n",
    "    \"\"\"Run one ML experiment with MLflow tracking.\"\"\"\n",
    "\n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "\n",
    "        # Log parameters\n",
    "        for k, v in params.items():\n",
    "            mlflow.log_param(k, v)\n",
    "\n",
    "        # Build pipeline\n",
    "        pipeline = Pipeline(stages=[\n",
    "            regex_tokenizer,\n",
    "            stop_remover,\n",
    "            hashing_tf,\n",
    "            idf,\n",
    "            label_indexer,\n",
    "            classifier\n",
    "        ])\n",
    "\n",
    "        # Train\n",
    "        model = pipeline.fit(train_df)\n",
    "\n",
    "        # Predict\n",
    "        preds = model.transform(test_df)\n",
    "\n",
    "        # Evaluate\n",
    "        f1 = evaluator.evaluate(preds)\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "\n",
    "        # Log model artifact\n",
    "        mlflow.spark.log_model(model, artifact_path=f\"{model_name}_model\")\n",
    "\n",
    "        print(f\"{model_name} | F1 = {f1:.4f}\")\n",
    "\n",
    "        return model, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c0d8bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_variants = [\n",
    "    (\"LR_reg_0.01\", LogisticRegression(labelCol=\"label_idx\", featuresCol=\"features\",\n",
    "                                       maxIter=20, regParam=0.01),\n",
    "     {\"regParam\": 0.01, \"maxIter\": 20}),\n",
    "\n",
    "    (\"LR_reg_0.1\", LogisticRegression(labelCol=\"label_idx\", featuresCol=\"features\",\n",
    "                                      maxIter=20, regParam=0.1),\n",
    "     {\"regParam\": 0.1, \"maxIter\": 20})\n",
    "]\n",
    "svm_variants = [\n",
    "    (\"SVM_OVR_reg_0.1\",\n",
    "     OneVsRest(\n",
    "         classifier=LinearSVC(\n",
    "             labelCol=\"label_idx\",\n",
    "             featuresCol=\"features\",\n",
    "             maxIter=20,\n",
    "             regParam=0.1\n",
    "         ),\n",
    "         labelCol=\"label_idx\",\n",
    "         featuresCol=\"features\"\n",
    "     ),\n",
    "     {\"model\": \"LinearSVC OVR\", \"regParam\": 0.1, \"maxIter\": 20}\n",
    "     ),\n",
    "\n",
    "    (\"SVM_OVR_reg_0.01\",\n",
    "     OneVsRest(\n",
    "         classifier=LinearSVC(\n",
    "             labelCol=\"label_idx\",\n",
    "             featuresCol=\"features\",\n",
    "             maxIter=30,\n",
    "             regParam=0.01\n",
    "         ),\n",
    "         labelCol=\"label_idx\",\n",
    "         featuresCol=\"features\"\n",
    "     ),\n",
    "     {\"model\": \"LinearSVC OVR\", \"regParam\": 0.01, \"maxIter\": 30}\n",
    "     )\n",
    "]\n",
    "nb_variants = [\n",
    "    (\"NB_smoothing_1\", NaiveBayes(labelCol=\"label_idx\", featuresCol=\"features\",\n",
    "                                  modelType=\"multinomial\", smoothing=1.0),\n",
    "     {\"smoothing\": 1.0}),\n",
    "\n",
    "    (\"NB_smoothing_0.5\", NaiveBayes(labelCol=\"label_idx\", featuresCol=\"features\",\n",
    "                                    modelType=\"multinomial\", smoothing=0.5),\n",
    "     {\"smoothing\": 0.5})\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a06f61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/07 21:15:06 WARN DAGScheduler: Broadcasting large task binary with size 1129.9 KiB\n",
      "25/12/07 21:15:10 WARN DAGScheduler: Broadcasting large task binary with size 1131.0 KiB\n",
      "25/12/07 21:15:11 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/12/07 21:15:11 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "25/12/07 21:15:11 WARN DAGScheduler: Broadcasting large task binary with size 1130.5 KiB\n",
      "25/12/07 21:15:13 WARN MemoryStore: Not enough space to cache rdd_50_5 in memory! (computed 17.0 MiB so far)\n",
      "25/12/07 21:15:13 WARN BlockManager: Persisting block rdd_50_5 to disk instead.\n",
      "25/12/07 21:15:13 WARN MemoryStore: Not enough space to cache rdd_50_1 in memory! (computed 17.0 MiB so far)\n",
      "25/12/07 21:15:13 WARN BlockManager: Persisting block rdd_50_1 to disk instead.\n",
      "25/12/07 21:15:14 WARN MemoryStore: Not enough space to cache rdd_50_7 in memory! (computed 33.0 MiB so far)\n",
      "25/12/07 21:15:14 WARN MemoryStore: Not enough space to cache rdd_50_3 in memory! (computed 33.0 MiB so far)\n",
      "25/12/07 21:15:14 WARN BlockManager: Persisting block rdd_50_7 to disk instead.\n",
      "25/12/07 21:15:14 WARN BlockManager: Persisting block rdd_50_3 to disk instead.\n",
      "25/12/07 21:15:16 WARN DAGScheduler: Broadcasting large task binary with size 1131.7 KiB\n",
      "25/12/07 21:15:16 WARN DAGScheduler: Broadcasting large task binary with size 1130.5 KiB\n",
      "25/12/07 21:15:16 WARN DAGScheduler: Broadcasting large task binary with size 1131.7 KiB\n",
      "25/12/07 21:15:16 WARN DAGScheduler: Broadcasting large task binary with size 1130.5 KiB\n",
      "25/12/07 21:15:16 WARN DAGScheduler: Broadcasting large task binary with size 1131.7 KiB\n",
      "25/12/07 21:15:16 WARN DAGScheduler: Broadcasting large task binary with size 1130.5 KiB\n",
      "25/12/07 21:15:17 WARN DAGScheduler: Broadcasting large task binary with size 1131.7 KiB\n",
      "25/12/07 21:15:17 WARN DAGScheduler: Broadcasting large task binary with size 1130.5 KiB\n",
      "25/12/07 21:15:17 WARN DAGScheduler: Broadcasting large task binary with size 1131.7 KiB\n",
      "25/12/07 21:15:17 WARN DAGScheduler: Broadcasting large task binary with size 1130.5 KiB\n",
      "25/12/07 21:15:17 WARN DAGScheduler: Broadcasting large task binary with size 1131.7 KiB\n",
      "25/12/07 21:15:17 WARN DAGScheduler: Broadcasting large task binary with size 1130.5 KiB\n",
      "25/12/07 21:15:17 WARN DAGScheduler: Broadcasting large task binary with size 1131.7 KiB\n",
      "25/12/07 21:15:17 WARN DAGScheduler: Broadcasting large task binary with size 1130.5 KiB\n",
      "25/12/07 21:15:17 WARN DAGScheduler: Broadcasting large task binary with size 1131.7 KiB\n",
      "25/12/07 21:15:17 WARN DAGScheduler: Broadcasting large task binary with size 1130.5 KiB\n",
      "25/12/07 21:15:18 WARN DAGScheduler: Broadcasting large task binary with size 1131.7 KiB\n",
      "25/12/07 21:15:18 WARN DAGScheduler: Broadcasting large task binary with size 1130.5 KiB\n",
      "25/12/07 21:15:18 WARN DAGScheduler: Broadcasting large task binary with size 1131.7 KiB\n",
      "25/12/07 21:15:18 WARN DAGScheduler: Broadcasting large task binary with size 1130.5 KiB\n",
      "25/12/07 21:15:18 WARN DAGScheduler: Broadcasting large task binary with size 1131.7 KiB\n",
      "25/12/07 21:15:18 WARN DAGScheduler: Broadcasting large task binary with size 1130.5 KiB\n",
      "25/12/07 21:15:18 WARN DAGScheduler: Broadcasting large task binary with size 1131.7 KiB\n",
      "25/12/07 21:15:18 WARN DAGScheduler: Broadcasting large task binary with size 1130.5 KiB\n",
      "25/12/07 21:15:18 WARN DAGScheduler: Broadcasting large task binary with size 1131.7 KiB\n",
      "25/12/07 21:15:18 WARN DAGScheduler: Broadcasting large task binary with size 1130.5 KiB\n",
      "25/12/07 21:15:18 WARN DAGScheduler: Broadcasting large task binary with size 1131.7 KiB\n",
      "25/12/07 21:15:18 WARN DAGScheduler: Broadcasting large task binary with size 1130.5 KiB\n",
      "25/12/07 21:15:19 WARN DAGScheduler: Broadcasting large task binary with size 1131.7 KiB\n",
      "25/12/07 21:15:19 WARN DAGScheduler: Broadcasting large task binary with size 1130.5 KiB\n",
      "25/12/07 21:15:19 WARN DAGScheduler: Broadcasting large task binary with size 1131.7 KiB\n",
      "25/12/07 21:15:19 WARN DAGScheduler: Broadcasting large task binary with size 1130.5 KiB\n",
      "25/12/07 21:15:19 WARN DAGScheduler: Broadcasting large task binary with size 1131.7 KiB\n",
      "25/12/07 21:15:19 WARN DAGScheduler: Broadcasting large task binary with size 1130.5 KiB\n",
      "25/12/07 21:15:19 WARN DAGScheduler: Broadcasting large task binary with size 1131.7 KiB\n",
      "25/12/07 21:15:19 WARN DAGScheduler: Broadcasting large task binary with size 1130.5 KiB\n",
      "25/12/07 21:15:19 WARN DAGScheduler: Broadcasting large task binary with size 1131.7 KiB\n",
      "25/12/07 21:15:19 WARN DAGScheduler: Broadcasting large task binary with size 1130.5 KiB\n",
      "25/12/07 21:15:19 WARN DAGScheduler: Broadcasting large task binary with size 1131.7 KiB\n",
      "25/12/07 21:15:19 WARN DAGScheduler: Broadcasting large task binary with size 1130.5 KiB\n",
      "25/12/07 21:15:20 WARN DAGScheduler: Broadcasting large task binary with size 1131.7 KiB\n",
      "25/12/07 21:15:20 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/12/07 21:15:22 WARN TaskSetManager: Stage 62 contains a task of very large size (1054 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/12/07 21:15:22 WARN TaskSetManager: Stage 70 contains a task of very large size (2099 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR_reg_0.01 | F1 = 0.8287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/07 21:15:37 WARN DAGScheduler: Broadcasting large task binary with size 1129.9 KiB\n",
      "25/12/07 21:15:41 WARN DAGScheduler: Broadcasting large task binary with size 1131.0 KiB\n",
      "25/12/07 21:15:41 WARN DAGScheduler: Broadcasting large task binary with size 1130.5 KiB\n",
      "25/12/07 21:15:43 WARN MemoryStore: Not enough space to cache rdd_217_1 in memory! (computed 17.0 MiB so far)\n",
      "25/12/07 21:15:43 WARN BlockManager: Persisting block rdd_217_1 to disk instead.\n",
      "25/12/07 21:15:43 WARN MemoryStore: Not enough space to cache rdd_217_5 in memory! (computed 17.0 MiB so far)\n",
      "25/12/07 21:15:43 WARN BlockManager: Persisting block rdd_217_5 to disk instead.\n",
      "25/12/07 21:15:44 WARN MemoryStore: Not enough space to cache rdd_217_7 in memory! (computed 33.0 MiB so far)\n",
      "25/12/07 21:15:44 WARN BlockManager: Persisting block rdd_217_7 to disk instead.\n",
      "25/12/07 21:15:44 WARN MemoryStore: Not enough space to cache rdd_217_3 in memory! (computed 33.0 MiB so far)\n",
      "25/12/07 21:15:44 WARN BlockManager: Persisting block rdd_217_3 to disk instead.\n",
      "25/12/07 21:15:46 WARN DAGScheduler: Broadcasting large task binary with size 1131.7 KiB\n",
      "25/12/07 21:15:46 WARN DAGScheduler: Broadcasting large task binary with size 1130.5 KiB\n",
      "25/12/07 21:15:47 WARN DAGScheduler: Broadcasting large task binary with size 1131.7 KiB\n",
      "25/12/07 21:15:47 WARN DAGScheduler: Broadcasting large task binary with size 1130.5 KiB\n",
      "25/12/07 21:15:47 WARN DAGScheduler: Broadcasting large task binary with size 1131.7 KiB\n",
      "25/12/07 21:15:47 WARN DAGScheduler: Broadcasting large task binary with size 1130.5 KiB\n",
      "25/12/07 21:15:47 WARN DAGScheduler: Broadcasting large task binary with size 1131.7 KiB\n",
      "25/12/07 21:15:47 WARN DAGScheduler: Broadcasting large task binary with size 1130.5 KiB\n",
      "25/12/07 21:15:47 WARN DAGScheduler: Broadcasting large task binary with size 1131.7 KiB\n",
      "25/12/07 21:15:47 WARN DAGScheduler: Broadcasting large task binary with size 1130.5 KiB\n",
      "25/12/07 21:15:47 WARN DAGScheduler: Broadcasting large task binary with size 1131.7 KiB\n",
      "25/12/07 21:15:47 WARN DAGScheduler: Broadcasting large task binary with size 1130.5 KiB\n",
      "25/12/07 21:15:47 WARN DAGScheduler: Broadcasting large task binary with size 1131.7 KiB\n",
      "25/12/07 21:15:47 WARN DAGScheduler: Broadcasting large task binary with size 1130.5 KiB\n",
      "25/12/07 21:15:48 WARN DAGScheduler: Broadcasting large task binary with size 1131.7 KiB\n",
      "25/12/07 21:15:48 WARN DAGScheduler: Broadcasting large task binary with size 1130.5 KiB\n",
      "25/12/07 21:15:48 WARN DAGScheduler: Broadcasting large task binary with size 1131.7 KiB\n",
      "25/12/07 21:15:48 WARN DAGScheduler: Broadcasting large task binary with size 1130.5 KiB\n",
      "25/12/07 21:15:48 WARN DAGScheduler: Broadcasting large task binary with size 1131.7 KiB\n",
      "25/12/07 21:15:48 WARN DAGScheduler: Broadcasting large task binary with size 1130.5 KiB\n",
      "25/12/07 21:15:48 WARN DAGScheduler: Broadcasting large task binary with size 1131.7 KiB\n",
      "25/12/07 21:15:48 WARN DAGScheduler: Broadcasting large task binary with size 1130.5 KiB\n",
      "25/12/07 21:15:48 WARN DAGScheduler: Broadcasting large task binary with size 1131.7 KiB\n",
      "25/12/07 21:15:48 WARN DAGScheduler: Broadcasting large task binary with size 1130.5 KiB\n",
      "25/12/07 21:15:48 WARN DAGScheduler: Broadcasting large task binary with size 1131.7 KiB\n",
      "25/12/07 21:15:48 WARN DAGScheduler: Broadcasting large task binary with size 1130.5 KiB\n",
      "25/12/07 21:15:49 WARN DAGScheduler: Broadcasting large task binary with size 1131.7 KiB\n",
      "25/12/07 21:15:49 WARN DAGScheduler: Broadcasting large task binary with size 1130.5 KiB\n",
      "25/12/07 21:15:49 WARN DAGScheduler: Broadcasting large task binary with size 1131.7 KiB\n",
      "25/12/07 21:15:49 WARN DAGScheduler: Broadcasting large task binary with size 1130.5 KiB\n",
      "25/12/07 21:15:49 WARN DAGScheduler: Broadcasting large task binary with size 1131.7 KiB\n",
      "25/12/07 21:15:49 WARN DAGScheduler: Broadcasting large task binary with size 1130.5 KiB\n",
      "25/12/07 21:15:49 WARN DAGScheduler: Broadcasting large task binary with size 1131.7 KiB\n",
      "25/12/07 21:15:49 WARN DAGScheduler: Broadcasting large task binary with size 1130.5 KiB\n",
      "25/12/07 21:15:49 WARN DAGScheduler: Broadcasting large task binary with size 1131.7 KiB\n",
      "25/12/07 21:15:49 WARN DAGScheduler: Broadcasting large task binary with size 1130.5 KiB\n",
      "25/12/07 21:15:49 WARN DAGScheduler: Broadcasting large task binary with size 1131.7 KiB\n",
      "25/12/07 21:15:49 WARN DAGScheduler: Broadcasting large task binary with size 1130.5 KiB\n",
      "25/12/07 21:15:49 WARN DAGScheduler: Broadcasting large task binary with size 1131.7 KiB\n",
      "25/12/07 21:15:50 WARN DAGScheduler: Broadcasting large task binary with size 1130.5 KiB\n",
      "25/12/07 21:15:50 WARN DAGScheduler: Broadcasting large task binary with size 1131.7 KiB\n",
      "25/12/07 21:15:50 WARN DAGScheduler: Broadcasting large task binary with size 1130.5 KiB\n",
      "25/12/07 21:15:50 WARN DAGScheduler: Broadcasting large task binary with size 1131.7 KiB\n",
      "25/12/07 21:15:50 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/12/07 21:15:52 WARN TaskSetManager: Stage 131 contains a task of very large size (1054 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/12/07 21:15:52 WARN TaskSetManager: Stage 139 contains a task of very large size (2099 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR_reg_0.1 | F1 = 0.8043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/07 21:16:04 WARN DAGScheduler: Broadcasting large task binary with size 1126.8 KiB\n",
      "25/12/07 21:16:09 WARN DAGScheduler: Broadcasting large task binary with size 1105.5 KiB\n",
      "25/12/07 21:16:09 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n",
      "25/12/07 21:16:11 WARN TaskSetManager: Stage 157 contains a task of very large size (1054 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/12/07 21:16:11 WARN TaskSetManager: Stage 165 contains a task of very large size (1576 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB_smoothing_1 | F1 = 0.8012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/07 21:16:24 WARN DAGScheduler: Broadcasting large task binary with size 1126.8 KiB\n",
      "25/12/07 21:16:29 WARN DAGScheduler: Broadcasting large task binary with size 1105.5 KiB\n",
      "25/12/07 21:16:29 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n",
      "25/12/07 21:16:31 WARN TaskSetManager: Stage 183 contains a task of very large size (1054 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/12/07 21:16:31 WARN TaskSetManager: Stage 191 contains a task of very large size (1576 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB_smoothing_0.5 | F1 = 0.8003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/07 21:16:44 WARN DAGScheduler: Broadcasting large task binary with size 1103.1 KiB\n",
      "25/12/07 21:16:49 WARN BlockManager: Block rdd_525_7 could not be removed as it was not found on disk or in memory\n",
      "25/12/07 21:16:49 ERROR Executor: Exception in task 7.0 in stage 202.0 (TID 1030)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:277)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:274)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:352)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1614)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$1468/0x000000a0018b27f0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1624/0x000000a001903918.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "25/12/07 21:16:49 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 7.0 in stage 202.0 (TID 1030),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:277)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:274)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:352)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1614)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$1468/0x000000a0018b27f0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1624/0x000000a001903918.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "25/12/07 21:16:49 WARN TaskSetManager: Lost task 7.0 in stage 202.0 (TID 1030) (192.168.1.66 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:277)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:274)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:352)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1614)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$1468/0x000000a0018b27f0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1624/0x000000a001903918.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\n",
      "25/12/07 21:16:49 ERROR TaskSetManager: Task 7 in stage 202.0 failed 1 times; aborting job\n",
      "25/12/07 21:16:49 WARN BlockManager: Putting block rdd_525_5 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/12/07 21:16:49 WARN BlockManager: Putting block rdd_525_3 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/12/07 21:16:49 WARN BlockManager: Putting block rdd_525_1 failed due to exception org.apache.spark.TaskKilledException.\n",
      "25/12/07 21:16:49 WARN BlockManager: Block rdd_525_5 could not be removed as it was not found on disk or in memory\n",
      "25/12/07 21:16:49 WARN BlockManager: Block rdd_525_1 could not be removed as it was not found on disk or in memory\n",
      "25/12/07 21:16:49 WARN BlockManager: Block rdd_525_3 could not be removed as it was not found on disk or in memory\n",
      "25/12/07 21:16:49 WARN TaskSetManager: Lost task 5.0 in stage 202.0 (TID 1028) (192.168.1.66 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 7 in stage 202.0 failed 1 times, most recent failure: Lost task 7.0 in stage 202.0 (TID 1030) (192.168.1.66 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:277)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:274)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:352)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1614)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$1468/0x000000a0018b27f0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1624/0x000000a001903918.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/12/07 21:16:49 WARN TaskSetManager: Lost task 1.0 in stage 202.0 (TID 1024) (192.168.1.66 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 7 in stage 202.0 failed 1 times, most recent failure: Lost task 7.0 in stage 202.0 (TID 1030) (192.168.1.66 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:277)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:274)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:352)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1614)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$1468/0x000000a0018b27f0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1624/0x000000a001903918.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/12/07 21:16:49 WARN TaskSetManager: Lost task 3.0 in stage 202.0 (TID 1026) (192.168.1.66 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 7 in stage 202.0 failed 1 times, most recent failure: Lost task 7.0 in stage 202.0 (TID 1030) (192.168.1.66 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:277)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:274)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:352)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1614)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$1468/0x000000a0018b27f0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1624/0x000000a001903918.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/12/07 21:16:49 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 202.0 failed 1 times, most recent failure: Lost task 7.0 in stage 202.0 (TID 1030) (192.168.1.66 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:277)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:274)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:352)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1614)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$1468/0x000000a0018b27f0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1624/0x000000a001903918.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2898)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2834)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2833)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2833)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1253)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1253)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1253)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3102)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3036)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3025)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:277)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:274)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:352)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1614)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$1468/0x000000a0018b27f0.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1624/0x000000a001903918.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\n",
      "25/12/07 21:16:49 ERROR Instrumentation: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "(No active SparkContext.)\n",
      "         \n",
      "\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n",
      "\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2702)\n",
      "\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.$anonfun$apply$1(CoalesceShufflePartitions.scala:61)\n",
      "\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:58)\n",
      "\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:34)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$optimizeQueryStage$2(AdaptiveSparkPlanExec.scala:169)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.optimizeQueryStage(AdaptiveSparkPlanExec.scala:168)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.newQueryStage(AdaptiveSparkPlanExec.scala:614)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:565)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:580)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:580)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:580)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:580)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:277)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:272)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:419)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:404)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:207)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:206)\n",
      "\tat org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:3851)\n",
      "\tat org.apache.spark.sql.Dataset.rdd(Dataset.scala:3849)\n",
      "\tat org.apache.spark.ml.util.Instrumentation.logDataset(Instrumentation.scala:62)\n",
      "\tat org.apache.spark.ml.classification.LinearSVC.$anonfun$train$1(LinearSVC.scala:174)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.classification.LinearSVC.train(LinearSVC.scala:172)\n",
      "\tat org.apache.spark.ml.classification.LinearSVC.train(LinearSVC.scala:77)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/leandrohermann/Library/CloudStorage/OneDrive-Personal/ITBA/Big Data/tp/itba-bigdata/.venv/lib/python3.13/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/leandrohermann/Library/CloudStorage/OneDrive-Personal/ITBA/Big Data/tp/itba-bigdata/.venv/lib/python3.13/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/leandrohermann/Library/CloudStorage/OneDrive-Personal/ITBA/Big Data/tp/itba-bigdata/.venv/lib/python3.13/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "        \"Error while sending or receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/leandrohermann/Library/CloudStorage/OneDrive-Personal/ITBA/Big Data/tp/itba-bigdata/.venv/lib/python3.13/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/leandrohermann/Library/CloudStorage/OneDrive-Personal/ITBA/Big Data/tp/itba-bigdata/.venv/lib/python3.13/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/leandrohermann/Library/CloudStorage/OneDrive-Personal/ITBA/Big Data/tp/itba-bigdata/.venv/lib/python3.13/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "        \"Error while sending or receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/k6/__6qnmds11x9ycwmmcbc9n940000gn/T/ipykernel_78813/1839678303.py\", line 57, in run_experiment\n",
      "    model = pipeline.fit(train_df)\n",
      "  File \"/Users/leandrohermann/Library/CloudStorage/OneDrive-Personal/ITBA/Big Data/tp/itba-bigdata/.venv/lib/python3.13/site-packages/pyspark/ml/base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "           ~~~~~~~~~^^^^^^^^^\n",
      "  File \"/Users/leandrohermann/Library/CloudStorage/OneDrive-Personal/ITBA/Big Data/tp/itba-bigdata/.venv/lib/python3.13/site-packages/pyspark/ml/pipeline.py\", line 134, in _fit\n",
      "    model = stage.fit(dataset)\n",
      "  File \"/Users/leandrohermann/Library/CloudStorage/OneDrive-Personal/ITBA/Big Data/tp/itba-bigdata/.venv/lib/python3.13/site-packages/pyspark/ml/base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "           ~~~~~~~~~^^^^^^^^^\n",
      "  File \"/Users/leandrohermann/Library/CloudStorage/OneDrive-Personal/ITBA/Big Data/tp/itba-bigdata/.venv/lib/python3.13/site-packages/pyspark/ml/classification.py\", line 3586, in _fit\n",
      "    models = pool.map(inheritable_thread_target(trainSingleClass), range(numClasses))\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/pool.py\", line 367, in map\n",
      "    return self._map_async(func, iterable, mapstar, chunksize).get()\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/pool.py\", line 774, in get\n",
      "    raise self._value\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "                    ~~~~^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/pool.py\", line 48, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/Users/leandrohermann/Library/CloudStorage/OneDrive-Personal/ITBA/Big Data/tp/itba-bigdata/.venv/lib/python3.13/site-packages/pyspark/util.py\", line 342, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/leandrohermann/Library/CloudStorage/OneDrive-Personal/ITBA/Big Data/tp/itba-bigdata/.venv/lib/python3.13/site-packages/pyspark/ml/classification.py\", line 3582, in trainSingleClass\n",
      "    return classifier.fit(trainingDataset, paramMap)\n",
      "           ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/leandrohermann/Library/CloudStorage/OneDrive-Personal/ITBA/Big Data/tp/itba-bigdata/.venv/lib/python3.13/site-packages/pyspark/ml/base.py\", line 203, in fit\n",
      "    return self.copy(params)._fit(dataset)\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^\n",
      "  File \"/Users/leandrohermann/Library/CloudStorage/OneDrive-Personal/ITBA/Big Data/tp/itba-bigdata/.venv/lib/python3.13/site-packages/pyspark/ml/wrapper.py\", line 381, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "  File \"/Users/leandrohermann/Library/CloudStorage/OneDrive-Personal/ITBA/Big Data/tp/itba-bigdata/.venv/lib/python3.13/site-packages/pyspark/ml/wrapper.py\", line 378, in _fit_java\n",
      "    return self._java_obj.fit(dataset._jdf)\n",
      "           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
      "  File \"/Users/leandrohermann/Library/CloudStorage/OneDrive-Personal/ITBA/Big Data/tp/itba-bigdata/.venv/lib/python3.13/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "        answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/Users/leandrohermann/Library/CloudStorage/OneDrive-Personal/ITBA/Big Data/tp/itba-bigdata/.venv/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/leandrohermann/Library/CloudStorage/OneDrive-Personal/ITBA/Big Data/tp/itba-bigdata/.venv/lib/python3.13/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "        \"An error occurred while calling {0}{1}{2}.\\n\".\n",
      "        format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/leandrohermann/Library/CloudStorage/OneDrive-Personal/ITBA/Big Data/tp/itba-bigdata/.venv/lib/python3.13/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/socket.py\", line 719, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ~~~~~~~~~~~~~~~~~~~~^^^\n",
      "ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/leandrohermann/Library/CloudStorage/OneDrive-Personal/ITBA/Big Data/tp/itba-bigdata/.venv/lib/python3.13/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/leandrohermann/Library/CloudStorage/OneDrive-Personal/ITBA/Big Data/tp/itba-bigdata/.venv/lib/python3.13/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "        \"Error while sending or receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mrun_experiment\u001b[39m\u001b[34m(model_name, classifier, params)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m model = \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# Predict\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-Personal/ITBA/Big Data/tp/itba-bigdata/.venv/lib/python3.13/site-packages/pyspark/ml/base.py:205\u001b[39m, in \u001b[36mEstimator.fit\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-Personal/ITBA/Big Data/tp/itba-bigdata/.venv/lib/python3.13/site-packages/pyspark/ml/pipeline.py:134\u001b[39m, in \u001b[36mPipeline._fit\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     model = \u001b[43mstage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m     transformers.append(model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-Personal/ITBA/Big Data/tp/itba-bigdata/.venv/lib/python3.13/site-packages/pyspark/ml/base.py:205\u001b[39m, in \u001b[36mEstimator.fit\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-Personal/ITBA/Big Data/tp/itba-bigdata/.venv/lib/python3.13/site-packages/pyspark/ml/classification.py:3586\u001b[39m, in \u001b[36mOneVsRest._fit\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m   3584\u001b[39m pool = ThreadPool(processes=\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m.getParallelism(), numClasses))\n\u001b[32m-> \u001b[39m\u001b[32m3586\u001b[39m models = \u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43minheritable_thread_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainSingleClass\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnumClasses\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3588\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m handlePersistence:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/pool.py:367\u001b[39m, in \u001b[36mPool.map\u001b[39m\u001b[34m(self, func, iterable, chunksize)\u001b[39m\n\u001b[32m    363\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m'''\u001b[39;00m\n\u001b[32m    364\u001b[39m \u001b[33;03mApply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[32m    365\u001b[39m \u001b[33;03min a list that is returned.\u001b[39;00m\n\u001b[32m    366\u001b[39m \u001b[33;03m'''\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/pool.py:774\u001b[39m, in \u001b[36mApplyResult.get\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/pool.py:125\u001b[39m, in \u001b[36mworker\u001b[39m\u001b[34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     result = (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/pool.py:48\u001b[39m, in \u001b[36mmapstar\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmapstar\u001b[39m(args):\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-Personal/ITBA/Big Data/tp/itba-bigdata/.venv/lib/python3.13/site-packages/pyspark/util.py:342\u001b[39m, in \u001b[36minheritable_thread_target.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    341\u001b[39m SparkContext._active_spark_context._jsc.sc().setLocalProperties(properties)\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-Personal/ITBA/Big Data/tp/itba-bigdata/.venv/lib/python3.13/site-packages/pyspark/ml/classification.py:3582\u001b[39m, in \u001b[36mOneVsRest._fit.<locals>.trainSingleClass\u001b[39m\u001b[34m(index)\u001b[39m\n\u001b[32m   3581\u001b[39m     paramMap[cast(HasWeightCol, classifier).weightCol] = weightCol\n\u001b[32m-> \u001b[39m\u001b[32m3582\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mclassifier\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainingDataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparamMap\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-Personal/ITBA/Big Data/tp/itba-bigdata/.venv/lib/python3.13/site-packages/pyspark/ml/base.py:203\u001b[39m, in \u001b[36mEstimator.fit\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m params:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-Personal/ITBA/Big Data/tp/itba-bigdata/.venv/lib/python3.13/site-packages/pyspark/ml/wrapper.py:381\u001b[39m, in \u001b[36mJavaEstimator._fit\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    380\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) -> JM:\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m     java_model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    382\u001b[39m     model = \u001b[38;5;28mself\u001b[39m._create_model(java_model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-Personal/ITBA/Big Data/tp/itba-bigdata/.venv/lib/python3.13/site-packages/pyspark/ml/wrapper.py:378\u001b[39m, in \u001b[36mJavaEstimator._fit_java\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    377\u001b[39m \u001b[38;5;28mself\u001b[39m._transfer_params_to_java()\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_java_obj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-Personal/ITBA/Big Data/tp/itba-bigdata/.venv/lib/python3.13/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-Personal/ITBA/Big Data/tp/itba-bigdata/.venv/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-Personal/ITBA/Big Data/tp/itba-bigdata/.venv/lib/python3.13/site-packages/py4j/protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31m<class 'str'>\u001b[39m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(61, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m mlflow.set_experiment(\u001b[33m\"\u001b[39m\u001b[33msentiment_baselines\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model_name, clf, params \u001b[38;5;129;01min\u001b[39;00m all_variants:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     model, f1 = \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     results[model_name] = f1\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAll results:\u001b[39m\u001b[33m\"\u001b[39m, results)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mrun_experiment\u001b[39m\u001b[34m(model_name, classifier, params)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_experiment\u001b[39m(model_name, classifier, params: \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m     38\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Run one ML experiment with MLflow tracking.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m mlflow.start_run(run_name=model_name):\n\u001b[32m     41\u001b[39m \n\u001b[32m     42\u001b[39m         \u001b[38;5;66;03m# Log parameters\u001b[39;00m\n\u001b[32m     43\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params.items():\n\u001b[32m     44\u001b[39m             mlflow.log_param(k, v)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-Personal/ITBA/Big Data/tp/itba-bigdata/.venv/lib/python3.13/site-packages/mlflow/tracking/fluent.py:260\u001b[39m, in \u001b[36mActiveRun.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_val, exc_tb)\u001b[39m\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(r.info.run_id == \u001b[38;5;28mself\u001b[39m.info.run_id \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m active_run_stack):\n\u001b[32m    259\u001b[39m     status = RunStatus.FINISHED \u001b[38;5;28;01mif\u001b[39;00m exc_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m RunStatus.FAILED\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m     \u001b[43mend_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunStatus\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatus\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m exc_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-Personal/ITBA/Big Data/tp/itba-bigdata/.venv/lib/python3.13/site-packages/mlflow/tracking/fluent.py:551\u001b[39m, in \u001b[36mend_run\u001b[39m\u001b[34m(status)\u001b[39m\n\u001b[32m    549\u001b[39m last_active_run_id = run.info.run_id\n\u001b[32m    550\u001b[39m _last_active_run_id.set(last_active_run_id)\n\u001b[32m--> \u001b[39m\u001b[32m551\u001b[39m \u001b[43mMlflowClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.set_terminated(last_active_run_id, status)\n\u001b[32m    552\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m last_active_run_id \u001b[38;5;129;01min\u001b[39;00m run_id_to_system_metrics_monitor:\n\u001b[32m    553\u001b[39m     system_metrics_monitor = run_id_to_system_metrics_monitor.pop(last_active_run_id)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-Personal/ITBA/Big Data/tp/itba-bigdata/.venv/lib/python3.13/site-packages/mlflow/tracking/client.py:217\u001b[39m, in \u001b[36mMlflowClient.__init__\u001b[39m\u001b[34m(self, tracking_uri, registry_uri)\u001b[39m\n\u001b[32m    206\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    207\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m    208\u001b[39m \u001b[33;03m    tracking_uri: Address of local or remote tracking server. If not provided, defaults\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    214\u001b[39m \u001b[33;03m        no such service was set, defaults to the tracking uri of the client.\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    216\u001b[39m final_tracking_uri = utils._resolve_tracking_uri(tracking_uri)\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m \u001b[38;5;28mself\u001b[39m._registry_uri = \u001b[43mregistry_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_resolve_registry_uri\u001b[49m\u001b[43m(\u001b[49m\u001b[43mregistry_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracking_uri\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[38;5;28mself\u001b[39m._tracking_client = TrackingServiceClient(final_tracking_uri)\n\u001b[32m    219\u001b[39m \u001b[38;5;28mself\u001b[39m._tracing_client = TracingClient(tracking_uri)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-Personal/ITBA/Big Data/tp/itba-bigdata/.venv/lib/python3.13/site-packages/mlflow/tracking/_model_registry/utils.py:189\u001b[39m, in \u001b[36m_resolve_registry_uri\u001b[39m\u001b[34m(registry_uri, tracking_uri)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_resolve_registry_uri\u001b[39m(\n\u001b[32m    182\u001b[39m     registry_uri: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m, tracking_uri: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    183\u001b[39m ) -> \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    184\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    185\u001b[39m \u001b[33;03m    Resolve the registry URI following the same logic as get_registry_uri().\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m    188\u001b[39m         registry_uri\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[43m_get_registry_uri_from_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    190\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _get_default_registry_uri_for_tracking_uri(_resolve_tracking_uri(tracking_uri))\n\u001b[32m    191\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-Personal/ITBA/Big Data/tp/itba-bigdata/.venv/lib/python3.13/site-packages/mlflow/tracking/_model_registry/utils.py:115\u001b[39m, in \u001b[36m_get_registry_uri_from_context\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _registry_uri \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _registry_uri\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m (uri := MLFLOW_REGISTRY_URI.get()) \u001b[38;5;129;01mor\u001b[39;00m (uri := \u001b[43m_get_registry_uri_from_spark_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[32m    116\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m uri\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _registry_uri\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-Personal/ITBA/Big Data/tp/itba-bigdata/.venv/lib/python3.13/site-packages/mlflow/tracking/_model_registry/utils.py:101\u001b[39m, in \u001b[36m_get_registry_uri_from_spark_session\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AnalysisException\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.mlflow.modelRegistryUri\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m AnalysisException:\n\u001b[32m    103\u001b[39m     \u001b[38;5;66;03m# In serverless clusters, session.conf.get() is unsupported\u001b[39;00m\n\u001b[32m    104\u001b[39m     \u001b[38;5;66;03m# and raises an AnalysisException. We may encounter this case\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    107\u001b[39m     \u001b[38;5;66;03m# check will have returned false (as of 2025-06-07, it checks\u001b[39;00m\n\u001b[32m    108\u001b[39m     \u001b[38;5;66;03m# an environment variable that isn't set by DBConnect)\u001b[39;00m\n\u001b[32m    109\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-Personal/ITBA/Big Data/tp/itba-bigdata/.venv/lib/python3.13/site-packages/pyspark/sql/conf.py:58\u001b[39m, in \u001b[36mRuntimeConfig.get\u001b[39m\u001b[34m(self, key, default)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     57\u001b[39m     \u001b[38;5;28mself\u001b[39m._checkType(default, \u001b[33m\"\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jconf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-Personal/ITBA/Big Data/tp/itba-bigdata/.venv/lib/python3.13/site-packages/py4j/java_gateway.py:1321\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1314\u001b[39m args_command, temp_args = \u001b[38;5;28mself\u001b[39m._build_args(*args)\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m-> \u001b[39m\u001b[32m1321\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1322\u001b[39m return_value = get_return_value(\n\u001b[32m   1323\u001b[39m     answer, \u001b[38;5;28mself\u001b[39m.gateway_client, \u001b[38;5;28mself\u001b[39m.target_id, \u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-Personal/ITBA/Big Data/tp/itba-bigdata/.venv/lib/python3.13/site-packages/py4j/java_gateway.py:1036\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry=\u001b[38;5;28;01mTrue\u001b[39;00m, binary=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1016\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[32m   1017\u001b[39m \u001b[33;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[32m   1018\u001b[39m \u001b[33;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1034\u001b[39m \u001b[33;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[32m   1035\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1038\u001b[39m         response = connection.send_command(command)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-Personal/ITBA/Big Data/tp/itba-bigdata/.venv/lib/python3.13/site-packages/py4j/clientserver.py:284\u001b[39m, in \u001b[36mJavaClient._get_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection.socket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-Personal/ITBA/Big Data/tp/itba-bigdata/.venv/lib/python3.13/site-packages/py4j/clientserver.py:291\u001b[39m, in \u001b[36mJavaClient._create_new_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    288\u001b[39m     connection = ClientServerConnection(\n\u001b[32m    289\u001b[39m         \u001b[38;5;28mself\u001b[39m.java_parameters, \u001b[38;5;28mself\u001b[39m.python_parameters,\n\u001b[32m    290\u001b[39m         \u001b[38;5;28mself\u001b[39m.gateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_thread_connection(connection)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-Personal/ITBA/Big Data/tp/itba-bigdata/.venv/lib/python3.13/site-packages/py4j/clientserver.py:438\u001b[39m, in \u001b[36mClientServerConnection.connect_to_java_server\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ssl_context:\n\u001b[32m    436\u001b[39m     \u001b[38;5;28mself\u001b[39m.socket = \u001b[38;5;28mself\u001b[39m.ssl_context.wrap_socket(\n\u001b[32m    437\u001b[39m         \u001b[38;5;28mself\u001b[39m.socket, server_hostname=\u001b[38;5;28mself\u001b[39m.java_address)\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[38;5;28mself\u001b[39m.stream = \u001b[38;5;28mself\u001b[39m.socket.makefile(\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    440\u001b[39m \u001b[38;5;28mself\u001b[39m.is_connected = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mConnectionRefusedError\u001b[39m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "all_variants = lr_variants + nb_variants + svm_variants\n",
    "\n",
    "\n",
    "results = {}\n",
    "mlflow.set_experiment(\"sentiment_baselines\")\n",
    "\n",
    "for model_name, clf, params in all_variants:\n",
    "    model, f1 = run_experiment(model_name, clf, params)\n",
    "    results[model_name] = f1\n",
    "\n",
    "print(\"All results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa911b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
