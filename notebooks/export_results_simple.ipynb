{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Results for TP Report (Simple Version)\n",
    "This notebook exports all metrics and creates visualizations for your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure MLflow\n",
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "client = MlflowClient()\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úÖ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all runs from sentiment_analysis experiment\n",
    "experiment = client.get_experiment_by_name(\"/Users/nmoccagatta@itba.edu.ar/sentiment_analysis\")\n",
    "\n",
    "if experiment:\n",
    "    runs = client.search_runs(\n",
    "        experiment_ids=[experiment.experiment_id],\n",
    "        order_by=[\"metrics.f1_score DESC\"]\n",
    "    )\n",
    "    \n",
    "    # Create comparison table\n",
    "    results = []\n",
    "    for run in runs:\n",
    "        results.append({\n",
    "            \"Model\": run.info.run_name,\n",
    "            \"F1 Score\": round(run.data.metrics.get(\"f1_score\", 0), 4),\n",
    "            \"Precision\": round(run.data.metrics.get(\"precision\", 0), 4),\n",
    "            \"Recall\": round(run.data.metrics.get(\"recall\", 0), 4)\n",
    "        })\n",
    "    \n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    print(f\"‚úÖ Found {len(df_results)} model runs\\n\")\n",
    "    print(f\"üèÜ Best Model: {df_results.iloc[0]['Model']}\")\n",
    "    print(f\"üìä Best F1 Score: {df_results.iloc[0]['F1 Score']}\\n\")\n",
    "    \n",
    "    display(df_results)\n",
    "else:\n",
    "    print(\"‚ùå Experiment not found\")\n",
    "    df_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualization 1: F1 Score Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_results is not None:\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    colors = ['#2ecc71' if i == 0 else '#3498db' for i in range(len(df_results))]\n",
    "    bars = plt.bar(df_results['Model'], df_results['F1 Score'], color=colors)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                 f'{height:.4f}',\n",
    "                 ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylabel('F1 Score (Macro)', fontsize=12)\n",
    "    plt.xlabel('Model', fontsize=12)\n",
    "    plt.title('Model Comparison - F1 Score Performance', fontsize=14, fontweight='bold')\n",
    "    plt.ylim(0, max(df_results['F1 Score']) * 1.1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üí° Right-click on the chart above and 'Save Image As' to download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualization 2: All Metrics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_results is not None:\n",
    "    metrics_df = df_results[['Model', 'F1 Score', 'Precision', 'Recall']].set_index('Model')\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    metrics_df.plot(kind='bar', ax=ax, color=['#e74c3c', '#3498db', '#2ecc71'])\n",
    "    \n",
    "    plt.ylabel('Score', fontsize=12)\n",
    "    plt.xlabel('Model', fontsize=12)\n",
    "    plt.title('Model Performance - All Metrics Comparison', fontsize=14, fontweight='bold')\n",
    "    plt.legend(title='Metrics', fontsize=10)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üí° Right-click on the chart above and 'Save Image As' to download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization 3: Model Type Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_results is not None:\n",
    "    # Categorize models by type\n",
    "    def categorize_model(model_name):\n",
    "        if 'LR' in model_name:\n",
    "            return 'Logistic Regression'\n",
    "        elif 'SVM' in model_name:\n",
    "            return 'Linear SVM'\n",
    "        elif 'NB' in model_name:\n",
    "            return 'Naive Bayes'\n",
    "        else:\n",
    "            return 'Other'\n",
    "    \n",
    "    df_results['Model Type'] = df_results['Model'].apply(categorize_model)\n",
    "    \n",
    "    # Calculate average by model type\n",
    "    model_type_avg = df_results.groupby('Model Type')[['F1 Score', 'Precision', 'Recall']].mean()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    model_type_avg.plot(kind='bar', ax=ax, color=['#e74c3c', '#3498db', '#2ecc71'])\n",
    "    \n",
    "    plt.ylabel('Average Score', fontsize=12)\n",
    "    plt.xlabel('Model Type', fontsize=12)\n",
    "    plt.title('Average Performance by Model Type', fontsize=14, fontweight='bold')\n",
    "    plt.legend(title='Metrics', fontsize=10)\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üí° Right-click on the chart above and 'Save Image As' to download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_results is not None:\n",
    "    summary = {\n",
    "        \"Total Models Trained\": len(df_results),\n",
    "        \"Best Model\": df_results.iloc[0]['Model'],\n",
    "        \"Best F1 Score\": df_results.iloc[0]['F1 Score'],\n",
    "        \"Best Precision\": df_results.iloc[0]['Precision'],\n",
    "        \"Best Recall\": df_results.iloc[0]['Recall'],\n",
    "        \"Average F1 Score\": round(df_results['F1 Score'].mean(), 4),\n",
    "        \"F1 Score Std Dev\": round(df_results['F1 Score'].std(), 4),\n",
    "        \"Best Model Type\": df_results.iloc[0]['Model Type']\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SUMMARY STATISTICS FOR YOUR TP REPORT\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    for key, value in summary.items():\n",
    "        print(f\"{key:25s}: {value}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Report Text Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_results is not None:\n",
    "    report_text = f\"\"\"\n",
    "RESULTADOS EXPERIMENTALES\n",
    "========================\n",
    "\n",
    "Se entrenaron {summary['Total Models Trained']} variantes de modelos de clasificaci√≥n para an√°lisis de sentimientos:\n",
    "- 3 variantes de Regresi√≥n Log√≠stica (C=1, 5, 10)\n",
    "- 4 variantes de SVM Lineal (C=0.1, 1, 10, 50)  \n",
    "- 4 variantes de Naive Bayes Multinomial (alpha=0.2, 0.5, 1, 2)\n",
    "\n",
    "MEJOR MODELO\n",
    "------------\n",
    "Modelo: {summary['Best Model']}\n",
    "Tipo: {summary['Best Model Type']}\n",
    "F1 Score (macro): {summary['Best F1 Score']}\n",
    "Precision (macro): {summary['Best Precision']}\n",
    "Recall (macro): {summary['Best Recall']}\n",
    "\n",
    "AN√ÅLISIS COMPARATIVO\n",
    "-------------------\n",
    "F1 Score promedio: {summary['Average F1 Score']}\n",
    "Desviaci√≥n est√°ndar: {summary['F1 Score Std Dev']}\n",
    "\n",
    "El modelo {summary['Best Model']} mostr√≥ el mejor desempe√±o con un F1 Score de {summary['Best F1 Score']},\n",
    "superando al promedio general por {round(summary['Best F1 Score'] - summary['Average F1 Score'], 4)} puntos.\n",
    "\n",
    "MLOPS IMPLEMENTADO\n",
    "------------------\n",
    "‚úÖ Tracking de experimentos con MLflow\n",
    "‚úÖ Arquitectura Medallion (Bronze ‚Üí Silver ‚Üí Gold)\n",
    "‚úÖ Registro de modelos en Unity Catalog (workspace.sentiment_analysis.sentiment_classifier)\n",
    "‚úÖ B√∫squeda sistem√°tica de hiperpar√°metros (12 variantes)\n",
    "‚úÖ Validaci√≥n estratificada 80/20\n",
    "‚úÖ Vectorizaci√≥n TF-IDF con 50,000 features y n-grams (1,2)\n",
    "‚úÖ M√©tricas de calidad de datos en cada etapa del pipeline\n",
    "\"\"\"\n",
    "    \n",
    "    print(report_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Data Table (Copy-Paste Ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_results is not None:\n",
    "    print(\"\\nüìã COPY THIS TABLE FOR YOUR REPORT:\\n\")\n",
    "    print(df_results.to_markdown(index=False))\n",
    "    print(\"\\nüí° This is in Markdown format - you can paste it directly into your report!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Instructions\n",
    "\n",
    "### To Save Charts:\n",
    "1. Right-click on each chart above\n",
    "2. Select \"Save Image As...\"\n",
    "3. Save as PNG files\n",
    "\n",
    "### To Save Data:\n",
    "1. Copy the markdown table from cell 7\n",
    "2. Copy the summary statistics from cell 5\n",
    "3. Copy the report text from cell 6\n",
    "\n",
    "### What to Include in Your Report:\n",
    "1. **Architecture Diagram** (from MLflow_Pipeline_Explanation.md)\n",
    "2. **Model Comparison Table** (from cell 7)\n",
    "3. **3 Charts** (saved from cells 2, 3, 4)\n",
    "4. **Summary Statistics** (from cell 5)\n",
    "5. **Screenshots** of MLflow Experiments and Registered Model\n",
    "6. **Your Analysis** using the template from cell 6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
